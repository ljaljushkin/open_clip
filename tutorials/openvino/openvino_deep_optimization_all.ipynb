{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, openvino\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nncf import NNCFConfig\n",
    "from nncf.common.logging import nncf_logger\n",
    "from nncf.torch import create_compressed_model, register_default_init_args\n",
    "from nncf.torch.initialization import PTInitializingDataLoader\n",
    "from nncf.torch.layer_utils import CompressionParameter\n",
    "\n",
    "from PIL import Image\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_transform, eval_transform = open_clip.create_model_and_transforms(\"ViT-B-16-plus-240\", pretrained=\"laion400m_e32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-B-16-plus-240')\n",
    "\n",
    "image = eval_transform(Image.open(\"../../docs/CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def get_pil_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "BACKUP_PAIR = (\n",
    "    get_pil_from_url(\n",
    "        \"https://thumbs.dreamstime.com/t/altai-mountains-mountain-lake-russia-siberia-chuya-ridge-49130812.jpg\"\n",
    "    ),\n",
    "    \"Altai mountains Stock Photography\",\n",
    ")\n",
    "AVAILABLE_EXAMPLES = []\n",
    "\n",
    "def check_text_data(data):\n",
    "    if isinstance(data, str):\n",
    "        return True\n",
    "    if isinstance(data, list):\n",
    "        return all(isinstance(x, str) for x in data)\n",
    "    return False    \n",
    "\n",
    "def laion2B_preprocess_train(examples, train_transforms, tokenize_captions, image_column=\"url\", text_column=\"caption\"):\n",
    "    url = examples[image_column]\n",
    "    try:\n",
    "        image = get_pil_from_url(url)\n",
    "        if not check_text_data(examples[text_column]):\n",
    "            raise ValueError(\"Text data is not valid\")\n",
    "        AVAILABLE_EXAMPLES.append((url, examples[text_column]))\n",
    "    except Exception:\n",
    "        print(f\"Can't load image from url: {url}, using cache with size: {len(AVAILABLE_EXAMPLES)}\")\n",
    "        if len(AVAILABLE_EXAMPLES) > 0:\n",
    "            backup_id = random.randint(0, len(AVAILABLE_EXAMPLES) - 1)\n",
    "            backup_example = AVAILABLE_EXAMPLES[backup_id]\n",
    "            try:\n",
    "                image = get_pil_from_url(backup_example[0])\n",
    "                examples[text_column] = backup_example[1]\n",
    "            except Exception:\n",
    "                print(f\"Can't load image from cached url: {backup_example[0]}, using backup\")\n",
    "                image = BACKUP_PAIR[0].copy()\n",
    "                examples[text_column] = BACKUP_PAIR[1]\n",
    "        else:\n",
    "            print(f\"Can't load image from url: {url}, using backup\")\n",
    "            image = BACKUP_PAIR[0].copy()\n",
    "            examples[text_column] = BACKUP_PAIR[1]\n",
    "\n",
    "    examples[\"pixel_values\"] = train_transforms(image)\n",
    "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "    return examples\n",
    "\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    caption_column = \"caption\"\n",
    "    captions = []\n",
    "    caption = examples[caption_column]\n",
    "    if isinstance(caption, str):\n",
    "        captions.append(caption)\n",
    "    elif isinstance(caption, (list, np.ndarray)):\n",
    "        # take a random caption if there are multiple\n",
    "        captions.append(random.choice(caption) if is_train else caption[0])\n",
    "    else:\n",
    "        raise ValueError(f\"Caption column `{caption_column}` should contain either strings or lists of strings.\")\n",
    "    #inputs = tokenizer(captions[0], max_length=tokenizer.model_max_length, padding=\"do_not_pad\", truncation=True)\n",
    "    #input_ids = inputs.input_ids\n",
    "    input_ids = tokenizer(captions[0])[0]\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "max_train_samples = 10#000\n",
    "dataset = load_dataset(\"laion/laion400m\", streaming=True)\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42, buffer_size=max_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_image(examples):\n",
    "    examples = [laion2B_preprocess_train(example, train_transform, tokenize_captions) for example in examples]\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    \n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def prepare_nncf_init_data(dataloader, init_steps):\n",
    "    nncf_init_data = []\n",
    "\n",
    "    print(f\"Fetching {init_steps} for the initialization...\")\n",
    "    for _, batch in tqdm(zip(range(init_steps), itertools.islice(dataloader, 0, init_steps))):\n",
    "        with torch.no_grad():\n",
    "            # Convert images to latent space\n",
    "            \n",
    "            nncf_init_data.append(\n",
    "                (\n",
    "                    batch[\"pixel_values\"].to(\"cpu\"),\n",
    "                    batch[\"input_ids\"].to(\"cpu\")\n",
    "                )\n",
    "            )\n",
    "    return nncf_init_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 1\n",
    "dataloader_num_workers = 4\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, collate_fn=collate_fn_image, batch_size=train_batch_size, num_workers=dataloader_num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 10 for the initialization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9d4d6895914b888de6cdcecb127fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't load image from url: http://criticsight.com/wp-content/uploads/2016/09/Gears-of-War-4-disponible-solo-en-XBOX-One-portada-criticsight-387x500.jpg, using cache with size: 1\n"
     ]
    }
   ],
   "source": [
    "opt_init_steps = 10\n",
    "init_data = prepare_nncf_init_data(train_dataloader, opt_init_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.init_data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.init_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.init_data[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply NNCF optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_encoder_config = {\n",
    "#     \"input_info\": [\n",
    "#         {  \n",
    "#             \"sample_size\": [1, 3, 240, 240]\n",
    "#         },\n",
    "#     ],\n",
    "#     \"log_dir\": \"./\",  # The log directory for NNCF-specific logging outputs.\n",
    "#     \"compression\": [\n",
    "#         {\n",
    "#             \"algorithm\": \"quantization\",  # Specify the algorithm here.\n",
    "#             \"preset\": \"mixed\",\n",
    "#             \"initializer\": {\n",
    "#                 \"range\": {\"num_init_samples\": opt_init_steps},\n",
    "#                 \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\": opt_init_steps},\n",
    "#             },\n",
    "#             \"scope_overrides\": {\"activations\": {\"{re}.*__matmul___0\": {\"mode\": \"symmetric\"}, \"{re}.*mean_0\": {\"mode\": \"symmetric\"}}},\n",
    "#             \"ignored_scopes\": [\n",
    "#                 \"{re}.*__add___.*\",\n",
    "#                 \"{re}.*layer_norm_0\",\n",
    "#                 \"{re}.*__truediv__*\",\n",
    "#                 # \"{re}.*__mul___.*\",\n",
    "#                 # \"{re}.*__matmul___1\",\n",
    "#             ],\n",
    "#             \"overflow_fix\": \"disable\",\n",
    "#             \"export_to_onnx_standard_ops\": True,\n",
    "#         },\n",
    "#     ],\n",
    "# }\n",
    "\n",
    "# init_dataloader = torch.utils.data.DataLoader(InitDataset(init_data), batch_size=1, num_workers=1)\n",
    "\n",
    "# class ImageEncoderInitDataLoader(PTInitializingDataLoader):\n",
    "#     def get_inputs(self, dataloader_output):\n",
    "#         image = dataloader_output[0].float().to(\"cpu\", non_blocking=True)\n",
    "#         return (image), {}\n",
    "\n",
    "#     def get_target(self, dataloader_output):\n",
    "#         return dataloader_output[0]\n",
    "\n",
    "# image_encoder_config = NNCFConfig.from_dict(image_encoder_config)\n",
    "# image_encoder_config = register_default_init_args(image_encoder_config, ImageEncoderInitDataLoader(init_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tomeov\n",
    "\n",
    "# tomeov.patch_openclip(model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, train_transform, eval_transform = open_clip.create_model_and_transforms(\"ViT-B-16-plus-240\", pretrained=\"laion400m_e32\")\n",
    "# image_controller, image_encoder = create_compressed_model(model.visual, image_encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_inputs_with_objwalk\n",
    "from nncf.config.structures import BNAdaptationInitArgs\n",
    "from nncf.config.structures import QuantizationRangeInitArgs\n",
    "\n",
    "text_encoder_config_dict = {\n",
    "    \"input_info\": [\n",
    "        {  \n",
    "            \"sample_size\": [77, 1, 640],\n",
    "            # batch_size, n_ctx, transformer.width\n",
    "        },\n",
    "        {\n",
    "            \"sample_size\": [77, 77],\n",
    "        }\n",
    "    ],\n",
    "    \"log_dir\": \"./\",  # The log directory for NNCF-specific logging outputs.\n",
    "    \"compression\": [\n",
    "        {\n",
    "            \"algorithm\": \"quantization\",  # Specify the algorithm here.\n",
    "            \"preset\": \"mixed\",\n",
    "            \"initializer\": {\n",
    "                \"range\": {\"num_init_samples\": opt_init_steps},\n",
    "                \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\": opt_init_steps},\n",
    "            },\n",
    "            \"scope_overrides\": {\"activations\": {\"{re}.*baddbmm_0\": {\"mode\": \"symmetric\"}}},\n",
    "            \"ignored_scopes\": [\n",
    "                \"{re}.*__add___.*\",\n",
    "                \"{re}.*layer_norm_.*\",\n",
    "                \"{re}.*__truediv__*\",\n",
    "                \"{re}.*/bmm_0\",\n",
    "            ],\n",
    "            \"overflow_fix\": \"disable\",\n",
    "            \"export_to_onnx_standard_ops\": True,\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "class TextEncoderInitDataLoader(PTInitializingDataLoader):\n",
    "    \"\"\"\n",
    "    This class wraps the nncf.Dataset.\n",
    "\n",
    "    This is required for proper initialization of certain compression algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_loader):\n",
    "        super().__init__(data_loader)\n",
    "        self._length = None\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._data_loader)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self._length is None:\n",
    "            data = self._data_loader\n",
    "            self._length = TextEncoderInitDataLoader._get_length(data)\n",
    "        return self._length\n",
    "\n",
    "    def get_inputs(self, dataloader_output):\n",
    "        with torch.no_grad():            \n",
    "            text_embeddings = dataloader_output[1].to(\"cpu\", non_blocking=True)\n",
    "            #text_embeddings = torch.squeeze(text_embeddings, 0)\n",
    "            print(f\"text_embeddings.shape: {text_embeddings.shape}\")\n",
    "            x = model.token_embedding(text_embeddings)\n",
    "            x = x + model.positional_embedding\n",
    "            print(f\"x.hape: {x.shape}\")\n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "            print(f\"x.hape: {x.shape}\")\n",
    "        return (x, \n",
    "                # torch.ones([77,77])\n",
    "                model.attn_mask\n",
    "                ), {}#{\"x\": x, \"attn_mask\": model.attn_mask}\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_length(iterable) -> int:\n",
    "        length = 0\n",
    "        for _ in iterable:\n",
    "            length = length + 1\n",
    "\n",
    "        return length\n",
    "\n",
    "# class TextEncoderInitDataLoader(PTInitializingDataLoader):\n",
    "#     def get_inputs(self, dataloader_output):\n",
    "#         with torch.no_grad():\n",
    "#             text_embeddings = dataloader_output[1].to(\"cpu\", non_blocking=True)\n",
    "#             text_embeddings = torch.squeeze(text_embeddings, 0)\n",
    "#             print(f\"text_embeddings.shape: {text_embeddings.shape}\")\n",
    "#             x = model.token_embedding(text_embeddings)\n",
    "#             x = x + model.positional_embedding\n",
    "#             print(f\"x.hape: {x.shape}\")\n",
    "#             x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "#             print(f\"x.hape: {x.shape}\")\n",
    "#         return (x, model.attn_mask), {}#{\"x\": x, \"attn_mask\": model.attn_mask}\n",
    "\n",
    "text_encoder_config = NNCFConfig.from_dict(text_encoder_config_dict)\n",
    "text_encoder_dataloader = TextEncoderInitDataLoader(init_data)\n",
    "# text_encoder_config = register_default_init_args(text_encoder_config, text_encoder_dataloader)\n",
    "text_encoder_config.register_extra_structs(\n",
    "        [\n",
    "            QuantizationRangeInitArgs(data_loader=text_encoder_dataloader),\n",
    "            BNAdaptationInitArgs(data_loader=text_encoder_dataloader),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, resblock in enumerate(model.transformer.resblocks):\n",
    "#     attn = tomeov.openclip.ToMeAttention(resblock.attn.embed_dim, resblock.attn.num_heads, qkv_bias=True)\n",
    "#     _, device = tomeov.openclip.convert_attention_block(resblock.attn, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Not adding activation input quantizer for operation: 2 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[0]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 16 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[0]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 26 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[0]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 27 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[0]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 31 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[0]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 32 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[1]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 46 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[1]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 56 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[1]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 57 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[1]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 61 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[1]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 62 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[2]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 76 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[2]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 86 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[2]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 87 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[2]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 91 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[2]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 92 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[3]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 106 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[3]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 116 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[3]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 117 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[3]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 121 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[3]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 122 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[4]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 136 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[4]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 146 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[4]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 147 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[4]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 151 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[4]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 152 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[5]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 166 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[5]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 176 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[5]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 177 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[5]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 181 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[5]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 182 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[6]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 196 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[6]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 206 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[6]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 207 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[6]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 211 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[6]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 212 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[7]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 226 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[7]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 236 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[7]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 237 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[7]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 241 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[7]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 242 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[8]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 256 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[8]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 266 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[8]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 267 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[8]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 271 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[8]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 272 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[9]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 286 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[9]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 296 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[9]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 297 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[9]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 301 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[9]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 302 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[10]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 316 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[10]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 326 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[10]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 327 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[10]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 331 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[10]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 332 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[11]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 346 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[11]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 356 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[11]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 357 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[11]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 361 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[11]/__add___1\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "INFO:nncf:Collecting tensor statistics |█               | 1 / 10\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "INFO:nncf:Collecting tensor statistics |███             | 2 / 10\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "INFO:nncf:Collecting tensor statistics |████            | 3 / 10\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "INFO:nncf:Collecting tensor statistics |██████          | 4 / 10\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "INFO:nncf:Collecting tensor statistics |████████        | 5 / 10\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "INFO:nncf:Collecting tensor statistics |█████████       | 6 / 10\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "INFO:nncf:Collecting tensor statistics |███████████     | 7 / 10\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "INFO:nncf:Collecting tensor statistics |████████████    | 8 / 10\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "INFO:nncf:Collecting tensor statistics |██████████████  | 9 / 10\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "INFO:nncf:Collecting tensor statistics |████████████████| 10 / 10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Statistics are not collected for TargetType.OPERATOR_PRE_HOOK 0 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[0]/MultiheadAttention[attn]/unsqueeze_0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 37\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_inputs_with_objwalk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_outputs_with_objwalk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# from nncf.torch.dynamic_graph.context import no_nncf_trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[39m# dummy_forward_fn = create_dummy_forward_fn(text_encoder_dataloader, get_model_device(model))\u001b[39;00m\n\u001b[1;32m     36\u001b[0m model, train_transform, eval_transform \u001b[39m=\u001b[39m open_clip\u001b[39m.\u001b[39mcreate_model_and_transforms(\u001b[39m\"\u001b[39m\u001b[39mViT-B-16-plus-240\u001b[39m\u001b[39m\"\u001b[39m, pretrained\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlaion400m_e32\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m text_controller, text_encoder \u001b[39m=\u001b[39m create_compressed_model(\n\u001b[1;32m     38\u001b[0m     model\u001b[39m.\u001b[39;49mtransformer, \n\u001b[1;32m     39\u001b[0m     text_encoder_config)\n\u001b[1;32m     40\u001b[0m     \u001b[39m# dummy_forward_fn=dummy_forward_fn,\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[39m# wrap_inputs_fn=wrap_inputs,\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[39m# wrap_outputs_fn=wrap_outputs)\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/nncf/nncf/telemetry/decorator.py:71\u001b[0m, in \u001b[0;36mtracked_function.__call__.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m events:\n\u001b[1;32m     64\u001b[0m         telemetry\u001b[39m.\u001b[39msend_event(\n\u001b[1;32m     65\u001b[0m             event_category\u001b[39m=\u001b[39mcategory,\n\u001b[1;32m     66\u001b[0m             event_action\u001b[39m=\u001b[39mevent\u001b[39m.\u001b[39mname,\n\u001b[1;32m     67\u001b[0m             event_label\u001b[39m=\u001b[39mevent\u001b[39m.\u001b[39mdata,\n\u001b[1;32m     68\u001b[0m             event_value\u001b[39m=\u001b[39mevent\u001b[39m.\u001b[39mint_data,\n\u001b[1;32m     69\u001b[0m         )\n\u001b[0;32m---> 71\u001b[0m retval \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m category \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m category \u001b[39m!=\u001b[39m previous_category:\n\u001b[1;32m     74\u001b[0m     telemetry\u001b[39m.\u001b[39mend_session(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_category)\n",
      "File \u001b[0;32m~/projects/nncf/nncf/torch/model_creation.py:117\u001b[0m, in \u001b[0;36mcreate_compressed_model\u001b[0;34m(model, config, compression_state, dummy_forward_fn, wrap_inputs_fn, wrap_outputs_fn, dump_graphs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m is_state_loadable:\n\u001b[1;32m    116\u001b[0m     builder\u001b[39m.\u001b[39mload_state(compression_state[BaseController\u001b[39m.\u001b[39mBUILDER_STATE])\n\u001b[0;32m--> 117\u001b[0m compressed_model \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39;49mapply_to(nncf_network)\n\u001b[1;32m    118\u001b[0m compression_ctrl \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39mbuild_controller(compressed_model)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m is_state_loadable:\n",
      "File \u001b[0;32m~/projects/nncf/nncf/torch/compression_method_api.py:123\u001b[0m, in \u001b[0;36mPTCompressionAlgorithmBuilder.apply_to\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_to\u001b[39m(\u001b[39mself\u001b[39m, model: NNCFNetwork) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NNCFNetwork:\n\u001b[1;32m    122\u001b[0m     transformer \u001b[39m=\u001b[39m PTModelTransformer(model)\n\u001b[0;32m--> 123\u001b[0m     transformation_layout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_transformation_layout(model)\n\u001b[1;32m    124\u001b[0m     transformed_model \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mtransform(transformation_layout)\n\u001b[1;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_init:\n",
      "File \u001b[0;32m~/projects/nncf/nncf/torch/compression_method_api.py:142\u001b[0m, in \u001b[0;36mPTCompressionAlgorithmBuilder.get_transformation_layout\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mApplies algorithm-specific modifications to the model. Hooks to be executed during model\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39mforward operation may be registered using NNCFNetwork command insertion methods. Additional\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m:return: NNCFNetwork with algorithm-specific modifications applied\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m check_scopes_in_graph(model\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39mget_original_graph(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignored_scopes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_scopes)\n\u001b[0;32m--> 142\u001b[0m layout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_transformation_layout(model)\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_frozen_layers(model)\n\u001b[1;32m    144\u001b[0m \u001b[39mreturn\u001b[39;00m layout\n",
      "File \u001b[0;32m~/projects/nncf/nncf/torch/quantization/algo.py:632\u001b[0m, in \u001b[0;36mQuantizationBuilder._get_transformation_layout\u001b[0;34m(self, target_model)\u001b[0m\n\u001b[1;32m    627\u001b[0m dup_filter \u001b[39m=\u001b[39m DuplicateFilter()  \u001b[39m# so that the overflow fix warning is only logged once\u001b[39;00m\n\u001b[1;32m    628\u001b[0m nncf_logger\u001b[39m.\u001b[39maddFilter(dup_filter)\n\u001b[1;32m    629\u001b[0m (\n\u001b[1;32m    630\u001b[0m     insertion_commands,\n\u001b[1;32m    631\u001b[0m     setup_to_module_id_translation_dict,\n\u001b[0;32m--> 632\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_insertion_commands_list_for_quantizer_setup(\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pt_quantizer_setup, target_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_minmax_values_for_range_init\n\u001b[1;32m    634\u001b[0m )\n\u001b[1;32m    635\u001b[0m nncf_logger\u001b[39m.\u001b[39mremoveFilter(dup_filter)\n\u001b[1;32m    637\u001b[0m transformation_layout \u001b[39m=\u001b[39m PTTransformationLayout()\n",
      "File \u001b[0;32m~/projects/nncf/nncf/torch/quantization/algo.py:964\u001b[0m, in \u001b[0;36mQuantizationBuilder._build_insertion_commands_list_for_quantizer_setup\u001b[0;34m(self, quantizer_setup, target_model, minmax_values_for_range_init)\u001b[0m\n\u001b[1;32m    961\u001b[0m     \u001b[39mif\u001b[39;00m minmax_stat \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m         range_init_minmax_values \u001b[39m=\u001b[39m (minmax_stat\u001b[39m.\u001b[39mmin_values, minmax_stat\u001b[39m.\u001b[39mmax_values)\n\u001b[0;32m--> 964\u001b[0m quantizer_module_id, commands \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_quantize_at_points_by_single_module(\n\u001b[1;32m    965\u001b[0m     target_model,\n\u001b[1;32m    966\u001b[0m     [\n\u001b[1;32m    967\u001b[0m         tp,\n\u001b[1;32m    968\u001b[0m     ],\n\u001b[1;32m    969\u001b[0m     qspec,\n\u001b[1;32m    970\u001b[0m     range_init_minmax_values,\n\u001b[1;32m    971\u001b[0m )\n\u001b[1;32m    973\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    974\u001b[0m     qp\u001b[39m.\u001b[39mis_weight_quantization_point()\n\u001b[1;32m    975\u001b[0m     \u001b[39mand\u001b[39;00m nncf_node\u001b[39m.\u001b[39mis_shared()\n\u001b[1;32m    976\u001b[0m     \u001b[39mand\u001b[39;00m nncf_node\u001b[39m.\u001b[39mlayer_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m already_weight_quantized_shared_layers\n\u001b[1;32m    977\u001b[0m ):\n\u001b[1;32m    978\u001b[0m     already_weight_quantized_shared_layers[nncf_node\u001b[39m.\u001b[39mlayer_name] \u001b[39m=\u001b[39m quantizer_module_id\n",
      "File \u001b[0;32m~/projects/nncf/nncf/torch/quantization/algo.py:1181\u001b[0m, in \u001b[0;36mQuantizationBuilder._quantize_at_points_by_single_module\u001b[0;34m(self, target_model, insertion_points, qspec, range_init_minmax_values)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     min_values \u001b[39m=\u001b[39m range_init_minmax_values[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtype(own_type)\n\u001b[1;32m   1179\u001b[0m     max_values \u001b[39m=\u001b[39m range_init_minmax_values[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtype(own_type)\n\u001b[0;32m-> 1181\u001b[0m     quantizer\u001b[39m.\u001b[39;49mapply_minmax_init(min_values\u001b[39m=\u001b[39;49mmin_values, max_values\u001b[39m=\u001b[39;49mmax_values, log_module_name\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m(primary_ip))\n\u001b[1;32m   1183\u001b[0m qids \u001b[39m=\u001b[39m []  \u001b[39m# type: List[QuantizerId]\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39mfor\u001b[39;00m ip \u001b[39min\u001b[39;00m insertion_points:\n",
      "File \u001b[0;32m~/projects/nncf/nncf/torch/quantization/layers.py:382\u001b[0m, in \u001b[0;36mBaseQuantizer.apply_minmax_init\u001b[0;34m(self, min_values, max_values, log_module_name)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mall(torch\u001b[39m.\u001b[39misinf(min_values)) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39mall(torch\u001b[39m.\u001b[39misinf(max_values)):\n\u001b[0;32m--> 382\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStatistics are not collected for \u001b[39m\u001b[39m{\u001b[39;00mlog_module_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    384\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39many(torch\u001b[39m.\u001b[39meq(min_values, np\u001b[39m.\u001b[39minf)) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39many(torch\u001b[39m.\u001b[39meq(max_values, \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf)):\n\u001b[1;32m    385\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSome of the values in statistics have infinite value for \u001b[39m\u001b[39m{\u001b[39;00mlog_module_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Statistics are not collected for TargetType.OPERATOR_PRE_HOOK 0 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[0]/MultiheadAttention[attn]/unsqueeze_0"
     ]
    }
   ],
   "source": [
    "# from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_inputs_with_objwalk\n",
    "# from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_outputs_with_objwalk\n",
    "# from nncf.torch.dynamic_graph.context import no_nncf_trace\n",
    "# from nncf.torch.nested_objects_traversal import objwalk\n",
    "# from nncf.torch.utils import is_tensor\n",
    "# from nncf.torch.dynamic_graph.io_handling import replicate_same_tensors\n",
    "# from nncf.torch.utils import get_model_device\n",
    "\n",
    "# def wrap_inputs(args, kwargs):\n",
    "#         return wrap_nncf_model_inputs_with_objwalk(args, kwargs)\n",
    "\n",
    "# def wrap_outputs(retval):\n",
    "#     return wrap_nncf_model_outputs_with_objwalk(retval)\n",
    "\n",
    "# def create_dummy_forward_fn(data_loader, device):\n",
    "#     def dummy_forward(model):\n",
    "#         with no_nncf_trace():\n",
    "#             data_item = next(iter(data_loader))\n",
    "#             args, kwargs = data_loader.get_inputs(data_item)\n",
    "\n",
    "#             def send_to_device(tensor):\n",
    "#                 return tensor.to(device)\n",
    "\n",
    "#             args = objwalk(args, is_tensor, send_to_device)\n",
    "#             kwargs = objwalk(kwargs, is_tensor, send_to_device)\n",
    "\n",
    "#         args, kwargs = wrap_inputs(args, kwargs)\n",
    "#         retval = model(*args, **kwargs)\n",
    "#         retval = replicate_same_tensors(retval)\n",
    "#         return wrap_outputs(retval)\n",
    "\n",
    "#     return dummy_forward\n",
    "\n",
    "# dummy_forward_fn = create_dummy_forward_fn(text_encoder_dataloader, get_model_device(model))\n",
    "\n",
    "model, train_transform, eval_transform = open_clip.create_model_and_transforms(\"ViT-B-16-plus-240\", pretrained=\"laion400m_e32\")\n",
    "text_controller, text_encoder = create_compressed_model(\n",
    "    model.transformer, \n",
    "    text_encoder_config)\n",
    "    # dummy_forward_fn=dummy_forward_fn,\n",
    "    # wrap_inputs_fn=wrap_inputs,\n",
    "    # wrap_outputs_fn=wrap_outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
